import numpy as np


# Linear Regression

class LinearRegression:
    def __init__(self, learning_rate=0.01, n_iter=1000):
        self.bias = None
        self.weights = None
        self.lr = learning_rate
        self.n_iter = n_iter

    def fit(self, X, y):  # X_train, y_train
        m, n = X.shape  # (number of samples, no. of features)

        # Step1 Initialize the values
        self.bias = 0
        self.weights = np.zeros(n)

        # Gradient Descent
        for i in range(self.n_iter):

            # Step2 (calc y_pred)
            y_pred = self.bias + np.dot(X, self.weights)

            # Step3 Calc Gradient
            db = (1/m) * np.sum(y_pred - y)  # derivative of bias
            dw = (1/m) * np.dot(X.T, (y_pred - y))

            # Step4 Update params
            self.bias -= self.lr * db
            self.weights -= self.lr * dw

    def predict(self, X):
        y_pred = self.bias + np.dot(X, self.weights)
        return y_pred


X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

model = LinearRegression()
model.fit(X, y)

y_pred = model.predict(X)
print(y_pred)
print(model.bias)
print(model.weights)


#LinearRegression With OLS

class LinearRegressionOLS:
    def __init__(self):
        self.bias = None
        self.weights = None

    def fit(self, X, y):
        m, n = X.shape

        X_b = np.c_[np.ones((m,1)), X]

        #Normal Equation
        # theta = np.dot(np.linalg.inv(np.dot(X_b.T, X_b)), np.dot(X_b.T, y)
        theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y 

        self.bias = theta[0]
        self.weights = theta[1:]
    def predict(self, X):
        y_pred = self.bias + np.dot(X, self.weights)
        return y_pred


X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

model = LinearRegressionOLS()
model.fit(X, y)

y_pred = model.predict(X)
print(y_pred)
print(model.bias)
print(model.weights)
